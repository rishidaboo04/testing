from haystack.components.converters import TextFileToDocument  # New converter for text files

# Update the input directory to point to the text files
input_dir = "txt_files"  # Update this folder name to point to your .txt folder

# Indexing function that runs only once
@st.cache_resource
def get_document_store():
    # Initialize document store
    document_store = InMemoryDocumentStore()

    # Initialize pipeline components
    file_type_router = FileTypeRouter(mime_types=["text/plain"])  # Handle .txt files
    text_converter = TextFileToDocument()  # Convert text files to Document objects
    document_joiner = DocumentJoiner()
    document_cleaner = DocumentCleaner()
    document_splitter = DocumentSplitter(split_by="word", split_length=150, split_overlap=50)
    document_embedder = SentenceTransformersDocumentEmbedder(model="sentence-transformers/all-MiniLM-L6-v2")
    document_writer = DocumentWriter(document_store)

    # Create the indexing pipeline
    preprocessing_pipeline = Pipeline()
    preprocessing_pipeline.add_component(instance=file_type_router, name="file_type_router")
    preprocessing_pipeline.add_component(instance=text_converter, name="text_converter")  # Text conversion
    preprocessing_pipeline.add_component(instance=document_joiner, name="document_joiner")
    preprocessing_pipeline.add_component(instance=document_cleaner, name="document_cleaner")
    preprocessing_pipeline.add_component(instance=document_splitter, name="document_splitter")
    preprocessing_pipeline.add_component(instance=document_embedder, name="document_embedder")
    preprocessing_pipeline.add_component(instance=document_writer, name="document_writer")

    # Connect pipeline components
    preprocessing_pipeline.connect("file_type_router.text/plain", "text_converter.sources")  # For .txt files
    preprocessing_pipeline.connect("text_converter", "document_joiner")
    preprocessing_pipeline.connect("document_joiner", "document_cleaner")
    preprocessing_pipeline.connect("document_cleaner", "document_splitter")
    preprocessing_pipeline.connect("document_splitter", "document_embedder")
    preprocessing_pipeline.connect("document_embedder", "document_writer")

    # Run the indexing pipeline
    preprocessing_pipeline.run({"file_type_router": {"sources": list(Path(input_dir).glob("**/*.txt"))}})  # .txt files
    return document_store

# Only display the spinner and success message temporarily
with st.spinner('Indexing documents...'):
    document_store = get_document_store()
st.empty()  # Clear the spinner once indexing completes
